version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2 # Use a version compatible with Kafka
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - flink-net

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      # Expose Kafka broker for external connections (e.g., producer from host)
      - "9092:9092"
      # Internal listener used by Flink and producer within the Docker network
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      # Listener configuration for internal and external access
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      # Topic settings
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      # Create the topic automatically if it doesn't exist
      KAFKA_CREATE_TOPICS: "crypto-tickers:1:1,crypto-aggregations:1:1"
    networks:
      - flink-net

  jobmanager:
    # build: . # Build using the default Dockerfile in the current directory
    build:
      context: .
      dockerfile: Dockerfile # Explicitly specify the Dockerfile for Flink app
    hostname: jobmanager
    container_name: jobmanager
    ports:
      - "8081:8081" # Flink UI
    command: jobmanager # Start Flink JobManager process
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: jobmanager
        # Add any other Flink config options here if needed
    depends_on:
      - kafka # Ensure Kafka is up before Flink cluster starts
    networks:
      - flink-net

  taskmanager:
    # build: . # Use the same image built for jobmanager
    build:
      context: .
      dockerfile: Dockerfile # Explicitly specify the Dockerfile for Flink app
    hostname: taskmanager # Unique hostname is good practice, though not strictly needed for one TM
    container_name: taskmanager
    depends_on:
      - jobmanager
      - kafka
    command: taskmanager # Start Flink TaskManager process
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
        # Add any other Flink config options here if needed
    networks:
      - flink-net

  # Service to submit the Flink job after the cluster is ready
  job-submitter:
    # build: .
    build:
      context: .
      dockerfile: Dockerfile # Explicitly specify the Dockerfile for Flink app
    container_name: job-submitter
    depends_on:
      - jobmanager
    # Command to wait for JobManager and then submit the JAR
    # It uses the JAR copied into the image at /opt/flink/usrlib/
    command: ["/opt/flink/submit-job.sh"] # Execute the script directly
    networks:
      - flink-net

  # NEW: Service for the Python data producer
  crypto-producer:
    build:
      context: . # Use the current directory as context
      dockerfile: producer.Dockerfile # Specify the new Dockerfile
    container_name: crypto-producer
    depends_on:
      - kafka # Ensure Kafka is ready before starting the producer
    environment:
      # Pass the internal Kafka broker address to the Python script
      KAFKA_BROKERS_INTERNAL: "kafka:9093"
    networks:
      - flink-net
    restart: on-failure # Optional: Restart the producer if it fails

  # NEW: Service for the Python live aggregation viewer
  live-agg-viewer:
    build:
      context: . # Use the current directory as context
      dockerfile: viewer.Dockerfile # Specify the new viewer Dockerfile
    container_name: live-agg-viewer
    depends_on:
      - kafka # Ensure Kafka is ready before starting the viewer
      - job-submitter # Optionally wait for the job to be submitted
    environment:
      # Ensure the viewer uses the internal Kafka address
      KAFKA_BROKER_VIEWER: "kafka:9093"
    networks:
      - flink-net
    restart: on-failure # Optional: Restart the viewer if it fails

networks:
  flink-net:
    driver: bridge 